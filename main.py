#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿è¡ŒæŒ‡å®šæ ¸å¿ƒäº‹ä»¶çš„å®Œæ•´çˆ¬å–æµç¨‹ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°è‡ªå®šä¹‰ç›®å½•/CSV
"""

from level1_scraper import Level1Scraper
import os
import csv
import argparse


def run_full_scrape(target_url: str, output_dir: str, csv_filename: str):
    scraper = Level1Scraper()
    try:
        if scraper.scrape_core_info(target_url) and scraper.scrape_sub_events(target_url):
            # è¦†ç›–é»˜è®¤çš„ä¿å­˜ä½ç½®åˆ° output_dir
            os.makedirs(output_dir, exist_ok=True)
            # ä¸€çº§é¡µé¢çˆ¬å–åç«‹å³ä¿å­˜
            scraper.save_data(os.path.join(output_dir, 'level1_data.json'))
            scraper.print_summary()

            # ç»„è£…CSVè¾“å‡ºæ–‡ä»¶è·¯å¾„
            csv_output = os.path.join(output_dir, csv_filename)

            # å¯åŠ¨äºŒçº§ï¼Œå®šå‘è¾“å‡º
            # äºŒçº§é¡µé¢ï¼šæ¯æ¡è¯„è®ºå®æ—¶ä¿å­˜ï¼ˆç”± Level2Scraper å®ç°ï¼‰ï¼Œå¹¶è¾“å‡ºåˆ°æŒ‡å®šç›®å½•
            scraper.start_level2_scraping(output_dir=output_dir, csv_output_file=csv_output)
        else:
            print('âŒ çˆ¬å–å¤±è´¥ï¼šæ— æ³•è·å–æ ¸å¿ƒä¿¡æ¯æˆ–å­äº‹ä»¶')
    finally:
        scraper.close()


if __name__ == '__main__':
    # ========== æ‰¹é‡æ¨¡å¼ï¼šåŸºäºCSVçš„urlåˆ—è¿›è¡Œæ‰¹é‡çˆ¬å– ==========
    # è¯·åœ¨è¿™é‡Œå¡«å†™CSVæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«è¡¨å¤´ï¼Œå¿…é¡»åŒ…å«åä¸º url çš„åˆ—ï¼‰
    # ç¤ºä¾‹ï¼šcsv_path = r'D:\web-agent\pachong\result_2025-05-01_to_2025-09-11.csv'
    csv_path = r'result_2025-05-01_to_2025-09-11.csv'  # <- åœ¨æ­¤å¡«å†™CSVè·¯å¾„

    # ä»CSVçš„ç¬¬å‡ è¡Œå¼€å§‹ï¼ˆ>=2ï¼Œå› ä¸ºç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼‰ï¼Œåˆ°ç¬¬å‡ è¡Œç»“æŸï¼ˆåŒ…å«ï¼Œç•™ç©ºåˆ™åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
    # ä¾‹å¦‚ï¼šstart_row = 2, end_row = 100 ä»£è¡¨å¤„ç†ç¬¬2è¡Œåˆ°ç¬¬100è¡Œ
    start_row = 2
    end_row = 4  # æˆ–è€…å¡«å†™å…·ä½“çš„è¡Œå·ï¼Œä¾‹å¦‚ 200

    # å‘½ä»¤è¡Œå‚æ•°/äº¤äº’å¼è¾“å…¥ï¼Œè¦†ç›–ä¸Šè¿°è¡Œå·é…ç½®
    parser = argparse.ArgumentParser(description='Batch scrape Baidu events from a CSV file of URLs.')
    parser.add_argument('--start-row', type=int, help='èµ·å§‹è¡Œå·ï¼ˆ>=2ï¼‰')
    parser.add_argument('--end-row', type=int, help='ç»“æŸè¡Œå·ï¼ˆå¯é€‰ï¼ŒåŒ…å«è¯¥è¡Œï¼‰')
    args, _unknown = parser.parse_known_args()

    if args.start_row is not None:
        start_row = args.start_row
    else:
        try:
            user_input = input(f'è¯·è¾“å…¥èµ·å§‹è¡Œå·(>=2ï¼Œé»˜è®¤ {start_row}): ').strip()
            if user_input:
                start_row = int(user_input)
        except Exception:
            pass

    if args.end_row is not None:
        end_row = args.end_row
    else:
        try:
            user_input = input('è¯·è¾“å…¥ç»“æŸè¡Œå·(å¯ç•™ç©ºï¼Œç•™ç©ºä»£è¡¨åˆ°æ–‡ä»¶æœ«å°¾): ').strip()
            if user_input:
                end_row = int(user_input)
        except Exception:
            pass

    if start_row is None or start_row < 2:
        print('âš ï¸ èµ·å§‹è¡Œå·æ— æ•ˆï¼Œå·²é‡ç½®ä¸º 2')
        start_row = 2

    if csv_path:
        # æ‰¹é‡è¯»å–å¹¶é€è¡Œçˆ¬å–
        with open(csv_path, 'r', encoding='utf-8', newline='') as f:
            reader = csv.DictReader(f)
            # å°†è¡Œå·ä¸æ•°æ®å¯¹é½ï¼šheaderåœ¨æ–‡ä»¶ç¬¬1è¡Œï¼Œæ•°æ®ä»ç¬¬2è¡Œå¼€å§‹
            for idx, row in enumerate(reader, start=2):
                if idx < start_row:
                    continue
                if end_row is not None and idx > end_row:
                    break

                url = (row.get('url') or '').strip()
                if not url:
                    print(f'â­ï¸ ç¬¬ {idx} è¡Œç¼ºå°‘ urlï¼Œå·²è·³è¿‡')
                    continue

                seq = idx - 1  # åºå· = è¡Œå· - 1ï¼ˆç¬¬2è¡Œå¯¹åº”åºå·1ï¼‰
                out_dir = os.path.join('data_BAI_DU', str(seq))
                out_csv_name = f'{seq}.csv'

                print(f'ğŸš€ å¼€å§‹å¤„ç† ç¬¬ {idx} è¡Œï¼ˆåºå· {seq}ï¼‰ï¼š{url}')
                try:
                    run_full_scrape(url, out_dir, out_csv_name)
                except Exception as e:
                    print(f'âŒ ç¬¬ {idx} è¡Œï¼ˆåºå· {seq}ï¼‰å¤„ç†å¤±è´¥ï¼š{e}')
        print('âœ… æ‰¹é‡å¤„ç†å®Œæˆ')
    else:
        # ========== å•ä¸ªæ¨¡å¼ï¼ˆä¿ç•™åŸåŠŸèƒ½ï¼ŒæŒ‰éœ€ä½¿ç”¨ï¼‰ ==========
        target_url = 'https://events.baidu.com/search/vein?platform=pc&record_id=708914&query=%E9%82%A3%E8%8B%B1%E8%80%81%E5%85%AC%E5%90%A6%E8%AE%A4%E5%87%BA%E8%BD%A8%3A%E5%9B%A0%E8%85%BF%E4%BC%A4%E8%A2%AB%E6%90%80%E6%89%B6%E4%B8%8A%E8%BD%A6&srcid=50367'
        output_dir = 'data/Cheating'
        csv_filename = 'Cheating.csv'
        run_full_scrape(target_url, output_dir, csv_filename)

        #    python main.py --start-row 2 --end-row 100