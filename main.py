#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç™¾åº¦äº‹ä»¶è¯„è®ºçˆ¬è™« - ä¸»æ‰§è¡Œè„šæœ¬
ä½¿ç”¨ä¼˜åŒ–åçš„çˆ¬è™«è¿›è¡Œå®Œæ•´çˆ¬å–
"""

import time
import os
import sys
from level1_scraper import Level1Scraper
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ä¼˜åŒ–åçš„çˆ¬è™«"""
    target_url = "https://events.baidu.com/search/vein?platform=pc&record_id=648521&query=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E6%8A%97%E6%97%A5%E6%88%98%E4%BA%89%E6%9A%A8%E4%B8%96%E7%95%8C%E5%8F%8D%E6%B3%95%E8%A5%BF%E6%96%AF%E6%88%98%E4%BA%89%E8%83%9C%E5%88%A980%E5%91%A8%E5%B9%B4%E7%BA%AA%E5%BF%B5%E6%97%A5&srcid=50367"
    
    scraper = Level1Scraper()
    try:
        # çˆ¬å–æ ¸å¿ƒä¿¡æ¯
        if scraper.scrape_core_info(target_url):
            # çˆ¬å–å­äº‹ä»¶
            if scraper.scrape_sub_events(target_url):
                scraper.save_data()
                print(f"\nâœ… ä¸€çº§ç•Œé¢çˆ¬å–å®Œæˆ!")
                print(f"ğŸ“Š æ ¸å¿ƒäº‹ä»¶: {scraper.core_info['core_event_name']}")
                print(f"ğŸ“Š æ›´æ–°æ—¶é—´: {scraper.core_info['update_time']}")
                print(f"ğŸ“Š å­äº‹ä»¶æ•°é‡: {scraper.core_info['sub_event_count']}")
                print(f"ğŸ“Š å®é™…æå–: {len(scraper.sub_events)} ä¸ªå­äº‹ä»¶")
                print(f"ğŸ“„ æ•°æ®ä¿å­˜: data/level1_data.json")
                
                # æ˜¾ç¤ºæ‘˜è¦
                scraper.print_summary()
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­çˆ¬å–è¯„è®º
                print(f"\nğŸ¤” æ˜¯å¦å¼€å§‹çˆ¬å–è¯„è®ºï¼Ÿ(y/n): ", end="")
                user_input = input().strip().lower()
                
                if user_input in ['y', 'yes', 'æ˜¯', '']:
                    print(f"\nğŸš€ å¼€å§‹çˆ¬å–è¯„è®º...")
                    total_comments = scraper.start_level2_scraping()
                    print(f"\nğŸ‰ å®Œæ•´çˆ¬å–å®Œæˆï¼")
                    print(f"ğŸ“Š æ€»è¯„è®ºæ•°: {total_comments}")
                    print(f"ğŸ“„ JSONæ–‡ä»¶: data/level2_data.json")
                    print(f"ğŸ“Š Excelæ–‡ä»¶: data/{scraper._sanitize_filename(scraper.core_info['core_event_name'])}_è¯„è®ºæ•°æ®.xlsx")
                else:
                    print(f"\nâ¸ï¸ è·³è¿‡è¯„è®ºçˆ¬å–")
            else:
                print("âŒ å­äº‹ä»¶çˆ¬å–å¤±è´¥")
        else:
            print("âŒ æ ¸å¿ƒä¿¡æ¯çˆ¬å–å¤±è´¥")
        
    finally:
        scraper.close()

if __name__ == "__main__":
    main()