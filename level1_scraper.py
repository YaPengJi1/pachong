#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç™¾åº¦äº‹ä»¶è¯„è®ºçˆ¬è™« - ä¸€çº§ç•Œé¢çˆ¬è™«
ä»ä¸»æ—¶é—´çº¿é¡µé¢æå–æ ¸å¿ƒä¿¡æ¯å’Œå­äº‹ä»¶åˆ—è¡¨
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
import os
from level2_scraper import Level2Scraper

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Level1Scraper:
    def __init__(self):
        self.session = requests.Session()
        self.driver = None
        self.core_info = {}
        self.sub_events = []
        self._init_session()
        self._init_selenium()
        self._ensure_data_dir()
    
    def _init_session(self):
        """åˆå§‹åŒ–requestsä¼šè¯"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.baidu.com/',
        })
    
    def _init_selenium(self):
        """åˆå§‹åŒ–Selenium"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-logging')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # å°è¯•åˆå§‹åŒ–WebDriver
            logger.info("æ­£åœ¨åˆå§‹åŒ–Chrome WebDriver...")
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(30)
            logger.info("Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Selenium WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriver")
            self.driver = None
    
    def _ensure_data_dir(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        if not os.path.exists('data'):
            os.makedirs('data')
            logger.info("åˆ›å»ºæ•°æ®ç›®å½•: data")
    
    def _sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        if not filename:
            return "æœªçŸ¥äº‹ä»¶"
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        illegal_chars = r'[<>:"/\\|?*]'
        filename = re.sub(illegal_chars, '_', filename)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 50:
            filename = filename[:50]
        return filename
    
    def scrape_core_info(self, url):
        """çˆ¬å–æ ¸å¿ƒä¿¡æ¯"""
        logger.info(f"å¼€å§‹çˆ¬å–æ ¸å¿ƒä¿¡æ¯: {url}")
        
        if not self.driver:
            logger.error("WebDriveræœªåˆå§‹åŒ–ï¼Œæ— æ³•çˆ¬å–")
            return False
        
        try:
            logger.info("æ­£åœ¨è®¿é—®é¡µé¢...")
            self.driver.get(url)
            
            logger.info("ç­‰å¾…é¡µé¢åŠ è½½...")
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            logger.info("é¡µé¢åŠ è½½å®Œæˆï¼Œå¼€å§‹è§£æ...")
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # 1. æ ¸å¿ƒäº‹ä»¶åç§°
            title_elem = soup.find('title')
            core_event_name = title_elem.get_text(strip=True) if title_elem else ''
            
            # 2. æœ€æ–°æ›´æ–°æ—¶é—´
            update_time_elem = soup.find('p', class_='create-time')
            update_time = update_time_elem.get_text(strip=True) if update_time_elem else ''
            
            # 3. å­äº‹ä»¶æ•°é‡ - ä»æ ‡ç­¾ä¸­è·å–
            sub_event_count = 0
            count_elem = soup.find('span', class_='count')
            if count_elem:
                try:
                    sub_event_count = int(count_elem.get_text(strip=True))
                except ValueError:
                    pass
            
            self.core_info = {
                'core_event_name': core_event_name,
                'update_time': update_time,
                'sub_event_count': sub_event_count,
                'scrape_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.info(f"æ ¸å¿ƒä¿¡æ¯æå–å®Œæˆ:")
            logger.info(f"  äº‹ä»¶åç§°: {core_event_name}")
            logger.info(f"  æ›´æ–°æ—¶é—´: {update_time}")
            logger.info(f"  å­äº‹ä»¶æ•°é‡: {sub_event_count}")
            
            return True
            
        except Exception as e:
            logger.error(f"çˆ¬å–æ ¸å¿ƒä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def scrape_sub_events(self, url):
        """çˆ¬å–164ä¸ªå­äº‹ä»¶"""
        logger.info("å¼€å§‹çˆ¬å–å­äº‹ä»¶åˆ—è¡¨...")
        
        try:
            self.driver.get(url)
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ç­‰å¾…åˆå§‹å†…å®¹åŠ è½½
            time.sleep(3)
            
            # ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥ä»¥åŠ è½½æ‰€æœ‰164ä¸ªäº‹ä»¶
            logger.info("æ­£åœ¨æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹...")
            
            # å…ˆç‚¹å‡»ä¸€æ¬¡æ—¶é—´æ’åºæŒ‰é’®ï¼ˆåªç‚¹å‡»ä¸€æ¬¡ï¼‰
            try:
                time_order_btn = self.driver.find_element(By.CSS_SELECTOR, ".btn-order")
                if time_order_btn.is_displayed():
                    logger.info("ç‚¹å‡»æ—¶é—´æ’åºæŒ‰é’®...")
                    self.driver.execute_script("arguments[0].click();", time_order_btn)
                    time.sleep(3)
            except:
                pass
            
            # æ»šåŠ¨åŠ è½½å†…å®¹
            for i in range(15):  # å‡å°‘æ»šåŠ¨æ¬¡æ•°
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´ç¡®ä¿å†…å®¹åŠ è½½
                
                # æ£€æŸ¥æ˜¯å¦æœ‰"åŠ è½½æ›´å¤š"æŒ‰é’®ï¼ˆæ’é™¤æ—¶é—´æ’åºæŒ‰é’®ï¼‰
                try:
                    load_buttons = self.driver.find_elements(By.CSS_SELECTOR, "button, .load-more, .more-btn, [class*='load'], [class*='more']")
                    for btn in load_buttons:
                        if btn.is_displayed() and ('åŠ è½½' in btn.text or 'æ›´å¤š' in btn.text or 'load' in btn.text.lower() or 'more' in btn.text.lower()):
                            logger.info(f"å‘ç°åŠ è½½æŒ‰é’®: {btn.text}")
                            self.driver.execute_script("arguments[0].click();", btn)
                            time.sleep(3)
                            break
                except:
                    pass
            
            # æœ€åç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½å®Œæˆ
            time.sleep(5)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            event_items = []
            selectors = [
                'div.item',
                'div[class*="item"]',
                'div[class*="event"]',
                'li[class*="item"]',
                'li[class*="event"]',
                '.timeline-item',
                '.event-item'
            ]
            
            for selector in selectors:
                items = soup.select(selector)
                if len(items) > len(event_items):
                    event_items = items
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(items)} ä¸ªäº‹ä»¶é¡¹")
            
            logger.info(f"æœ€ç»ˆæ‰¾åˆ° {len(event_items)} ä¸ªäº‹ä»¶é¡¹")
            
            for i, item in enumerate(event_items):
                try:
                    sub_event = self._extract_event_from_item(item, i+1)
                    if sub_event:
                        self.sub_events.append(sub_event)
                except Exception as e:
                    logger.warning(f"è§£æäº‹ä»¶é¡¹ {i+1} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"æˆåŠŸæå– {len(self.sub_events)} ä¸ªå­äº‹ä»¶")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå­äº‹ä»¶é¢„è§ˆ
            for i, event in enumerate(self.sub_events[:5]):
                logger.info(f"å­äº‹ä»¶ {i+1}: {event['title'][:50]}... - {event['time']}")
            
            return True
            
        except Exception as e:
            logger.error(f"çˆ¬å–å­äº‹ä»¶å¤±è´¥: {e}")
            return False
    
    def _extract_event_from_item(self, item, index):
        """ä»å•ä¸ªitemå…ƒç´ ä¸­æå–äº‹ä»¶ä¿¡æ¯"""
        sub_event = {
            'id': f'event_{index}',
            'title': '',
            'link': '',
            'time': '',
            'summary': '',
            'author': ''
        }
        
        # æå–æ—¶é—´
        time_elem = item.find('span', class_='time')
        if time_elem:
            sub_event['time'] = time_elem.get_text(strip=True)
        
        # æå–æ ‡é¢˜å’Œé“¾æ¥
        title_link = item.find('a', class_='content-link')
        if title_link:
            sub_event['title'] = title_link.get_text(strip=True)
            sub_event['link'] = title_link.get('href', '')
        
        # æå–æ‘˜è¦å’Œä½œè€…
        dynamic_container = item.find('a', class_='dynamic-container')
        if dynamic_container:
            # æå–ä½œè€…
            author_elem = dynamic_container.find('div', class_='dynamic-author')
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                sub_event['author'] = author_text.replace('ï¼š', '').replace(':', '')
            
            # æå–æ‘˜è¦
            content_elem = dynamic_container.find('div', class_='dynamic-content')
            if content_elem:
                sub_event['summary'] = content_elem.get_text(strip=True)
        
        # åªæœ‰å½“æ ‡é¢˜ä¸ä¸ºç©ºæ—¶æ‰è¿”å›
        if sub_event['title']:
            return sub_event
        
        return None
    
    def save_data(self, filename='data/level1_data.json'):
        """ä¿å­˜æ•°æ®"""
        try:
            output_data = {
                'core_info': self.core_info,
                'sub_events': self.sub_events,
                'total_sub_events': len(self.sub_events),
                'scrape_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def start_level2_scraping(self):
        """å¯åŠ¨äºŒçº§è¯„è®ºçˆ¬å–"""
        logger.info("å¼€å§‹å¯åŠ¨äºŒçº§è¯„è®ºçˆ¬å–...")
        
        try:
            # åˆ›å»ºäºŒçº§çˆ¬è™«å®ä¾‹
            level2_scraper = Level2Scraper(self.core_info.get('core_event_name', ''))
            
            # å¼€å§‹çˆ¬å–è¯„è®º
            total_comments = level2_scraper.scrape_all_comments(self.sub_events)
            
            # æ˜¾ç¤ºæ‘˜è¦
            level2_scraper.print_summary()
            
            # å…³é—­èµ„æº
            level2_scraper.close()
            
            logger.info(f"äºŒçº§è¯„è®ºçˆ¬å–å®Œæˆï¼Œå…±è·å– {total_comments} æ¡è¯„è®º")
            return total_comments
            
        except Exception as e:
            logger.error(f"äºŒçº§è¯„è®ºçˆ¬å–å¤±è´¥: {e}")
            return 0
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ä¸€çº§ç•Œé¢çˆ¬å–ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ ¸å¿ƒäº‹ä»¶: {self.core_info.get('core_event_name', 'æœªè·å–')}")
        print(f"æ›´æ–°æ—¶é—´: {self.core_info.get('update_time', 'æœªè·å–')}")
        print(f"å­äº‹ä»¶æ•°é‡: {self.core_info.get('sub_event_count', 0)}")
        print(f"å®é™…æå–: {len(self.sub_events)} ä¸ªå­äº‹ä»¶")
        
        if self.sub_events:
            print(f"\nå‰5ä¸ªå­äº‹ä»¶:")
            for i, event in enumerate(self.sub_events[:5]):
                print(f"{i+1}. {event['time']} - {event['title'][:50]}...")
                if event['author']:
                    print(f"   ä½œè€…: {event['author']}")
                if event['summary']:
                    print(f"   æ‘˜è¦: {event['summary'][:80]}...")
                print()
        
        print("="*60)
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.driver:
            self.driver.quit()
        self.session.close()

def main():
    """ä¸»å‡½æ•°"""
    target_url = "https://events.baidu.com/search/vein?platform=pc&record_id=648521&query=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E6%8A%97%E6%97%A5%E6%88%98%E4%BA%89%E6%9A%A8%E4%B8%96%E7%95%8C%E5%8F%8D%E6%B3%95%E8%A5%BF%E6%96%AF%E6%88%98%E4%BA%89%E8%83%9C%E5%88%A980%E5%91%A8%E5%B9%B4%E7%BA%AA%E5%BF%B5%E6%97%A5&srcid=50367"
    
    scraper = Level1Scraper()
    try:
        # çˆ¬å–æ ¸å¿ƒä¿¡æ¯
        if scraper.scrape_core_info(target_url):
            # çˆ¬å–å­äº‹ä»¶
            if scraper.scrape_sub_events(target_url):
                scraper.save_data()
                print(f"\nâœ… ä¸€çº§ç•Œé¢çˆ¬å–å®Œæˆ!")
                print(f"ğŸ“Š æ ¸å¿ƒäº‹ä»¶: {scraper.core_info['core_event_name']}")
                print(f"ğŸ“Š æ›´æ–°æ—¶é—´: {scraper.core_info['update_time']}")
                print(f"ğŸ“Š å­äº‹ä»¶æ•°é‡: {scraper.core_info['sub_event_count']}")
                print(f"ğŸ“Š å®é™…æå–: {len(scraper.sub_events)} ä¸ªå­äº‹ä»¶")
                print(f"ğŸ“„ æ•°æ®ä¿å­˜: data/level1_data.json")
                
                # æ˜¾ç¤ºæ‘˜è¦
                scraper.print_summary()
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­çˆ¬å–è¯„è®º
                print(f"\nğŸ¤” æ˜¯å¦å¼€å§‹çˆ¬å–è¯„è®ºï¼Ÿ(y/n): ", end="")
                user_input = input().strip().lower()
                
                if user_input in ['y', 'yes', 'æ˜¯', '']:
                    print(f"\nğŸš€ å¼€å§‹çˆ¬å–è¯„è®º...")
                    total_comments = scraper.start_level2_scraping()
                    print(f"\nğŸ‰ å®Œæ•´çˆ¬å–å®Œæˆï¼")
                    print(f"ğŸ“Š æ€»è¯„è®ºæ•°: {total_comments}")
                    print(f"ğŸ“„ JSONæ–‡ä»¶: data/level2_data.json")
                    print(f"ğŸ“Š Excelæ–‡ä»¶: data/{scraper._sanitize_filename(scraper.core_info['core_event_name'])}_è¯„è®ºæ•°æ®.xlsx")
                else:
                    print(f"\nâ¸ï¸ è·³è¿‡è¯„è®ºçˆ¬å–")
            else:
                print("âŒ å­äº‹ä»¶çˆ¬å–å¤±è´¥")
        else:
            print("âŒ æ ¸å¿ƒä¿¡æ¯çˆ¬å–å¤±è´¥")
        
    finally:
        scraper.close()

if __name__ == "__main__":
    main()
