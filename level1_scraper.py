#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç™¾åº¦äº‹ä»¶è¯„è®ºçˆ¬è™« - ä¸€çº§ç•Œé¢çˆ¬è™«
ä»ä¸»æ—¶é—´çº¿é¡µé¢æå–æ ¸å¿ƒä¿¡æ¯å’Œå­äº‹ä»¶åˆ—è¡¨
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.edge.options import Options as EdgeOptions
from selenium.webdriver.edge.service import Service as EdgeService
import os
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
import os
from level2_scraper import Level2Scraper
from data_manager import DataManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Level1Scraper:
    def __init__(self):
        self.session = requests.Session()
        self.driver = None
        self.core_info = {}
        self.sub_events = []
        self._init_session()
        self._init_selenium()
        self._ensure_data_dir()
    
    def _init_session(self):
        """åˆå§‹åŒ–requestsä¼šè¯"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.baidu.com/',
        })
    
    def _init_selenium(self):
        """åˆå§‹åŒ–Selenium"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-logging')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # ä¼˜å…ˆå°è¯•ï¼šSelenium Managerï¼ˆä¸ä¸‹è½½ç¬¬ä¸‰æ–¹ä¾èµ–ï¼‰
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ–Chromeï¼ˆSelenium Managerï¼‰...")
                self.driver = webdriver.Chrome(options=chrome_options)
                self.driver.set_page_load_timeout(30)
                logger.info("Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ (Chrome)")
                return
            except Exception as e1:
                logger.warning(f"Chrome (Selenium Manager) åˆå§‹åŒ–å¤±è´¥: {e1}")

            # å¤‡ç”¨æ–¹æ¡ˆï¼šæœ¬åœ°é©±åŠ¨ï¼ˆä¸è¿›è¡Œç½‘ç»œä¸‹è½½ï¼‰
            try:
                logger.info("å°è¯•ä½¿ç”¨æœ¬åœ°chromedriverï¼ˆè·³è¿‡ç½‘ç»œä¸‹è½½ï¼‰...")
                os.environ['WDM_LOCAL'] = '1'  # ç¦æ­¢webdriver-managerè”ç½‘ä¸‹è½½ï¼Œè‹¥æ— æœ¬åœ°ç¼“å­˜å°†å¿«é€Ÿå¤±è´¥
                chromedriver_path = os.environ.get('CHROMEDRIVER_PATH', '')
                if chromedriver_path and os.path.exists(chromedriver_path):
                    logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡CHROMEDRIVER_PATH: {chromedriver_path}")
                    self.driver = webdriver.Chrome(service=ChromeService(chromedriver_path), options=chrome_options)
                    self.driver.set_page_load_timeout(30)
                    logger.info("Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ (æœ¬åœ°chromedriver)")
                    return
            except Exception as e2:
                logger.warning(f"æœ¬åœ°chromedriver åˆå§‹åŒ–å¤±è´¥: {e2}")

            # æœ€åå¤‡ç”¨ï¼šMicrosoft Edgeï¼ˆWindowsæ›´æ˜“å¯ç”¨ï¼‰
            try:
                logger.info("å°è¯•ä½¿ç”¨Edge WebDriver åˆå§‹åŒ–...")
                edge_options = EdgeOptions()
                edge_options.use_chromium = True
                edge_options.add_argument('--headless')
                edge_options.add_argument('--no-sandbox')
                edge_options.add_argument('--disable-dev-shm-usage')
                edge_options.add_argument('--disable-gpu')
                edge_options.add_argument('--disable-extensions')
                edge_options.add_argument('--disable-logging')
                edge_options.add_argument('--disable-web-security')
                edge_options.add_argument('--window-size=1920,1080')
                self.driver = webdriver.Edge(options=edge_options)
                self.driver.set_page_load_timeout(30)
                logger.info("Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ (Edge)")
                return
            except Exception as e3:
                logger.error(f"Edge åˆå§‹åŒ–å¤±è´¥: {e3}")

            # å…¨éƒ¨å¤±è´¥
            raise RuntimeError("æ— æ³•åˆå§‹åŒ–ä»»ä½•æµè§ˆå™¨é©±åŠ¨ã€‚è¯·å®‰è£… Chrome/Edge æˆ–æä¾› CHROMEDRIVER_PATHã€‚")
        except Exception as e:
            logger.error(f"Selenium WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriver")
            self.driver = None
    
    def _ensure_data_dir(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        if not os.path.exists('data'):
            os.makedirs('data')
            logger.info("åˆ›å»ºæ•°æ®ç›®å½•: data")
    
    def _sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        if not filename:
            return "æœªçŸ¥äº‹ä»¶"
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        illegal_chars = r'[<>:"/\\|?*]'
        filename = re.sub(illegal_chars, '_', filename)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 50:
            filename = filename[:50]
        return filename
    
    def scrape_core_info(self, url):
        """çˆ¬å–æ ¸å¿ƒä¿¡æ¯"""
        logger.info(f"å¼€å§‹çˆ¬å–æ ¸å¿ƒä¿¡æ¯: {url}")
        
        if not self.driver:
            logger.error("WebDriveræœªåˆå§‹åŒ–ï¼Œæ— æ³•çˆ¬å–")
            return False
        
        try:
            logger.info("æ­£åœ¨è®¿é—®é¡µé¢...")
            self.driver.get(url)
            
            logger.info("ç­‰å¾…é¡µé¢åŠ è½½...")
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            logger.info("é¡µé¢åŠ è½½å®Œæˆï¼Œå¼€å§‹è§£æ...")
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # 1. æ ¸å¿ƒäº‹ä»¶åç§°
            title_elem = soup.find('title')
            core_event_name = title_elem.get_text(strip=True) if title_elem else ''
            
            # 2. æœ€æ–°æ›´æ–°æ—¶é—´
            update_time_elem = soup.find('p', class_='create-time')
            update_time = update_time_elem.get_text(strip=True) if update_time_elem else ''
            
            # 3. å­äº‹ä»¶æ•°é‡ - ä»æ ‡ç­¾ä¸­è·å–
            sub_event_count = 0
            count_elem = soup.find('span', class_='count')
            if count_elem:
                try:
                    sub_event_count = int(count_elem.get_text(strip=True))
                except ValueError:
                    pass
            
            self.core_info = {
                'core_event_name': core_event_name,
                'update_time': update_time,
                'sub_event_count': sub_event_count,
                'scrape_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.info(f"æ ¸å¿ƒä¿¡æ¯æå–å®Œæˆ:")
            logger.info(f"  äº‹ä»¶åç§°: {core_event_name}")
            logger.info(f"  æ›´æ–°æ—¶é—´: {update_time}")
            logger.info(f"  å­äº‹ä»¶æ•°é‡: {sub_event_count}")
            
            return True
            
        except Exception as e:
            logger.error(f"çˆ¬å–æ ¸å¿ƒä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def scrape_sub_events(self, url):
        """çˆ¬å–164ä¸ªå­äº‹ä»¶"""
        logger.info("å¼€å§‹çˆ¬å–å­äº‹ä»¶åˆ—è¡¨...")
        
        try:
            self.driver.get(url)
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ç­‰å¾…åˆå§‹å†…å®¹åŠ è½½
            time.sleep(3)
            
            # åŠ¨æ€åŠ è½½ï¼šå¾ªç¯æ»šåŠ¨ + ç‚¹å‡»â€œåŠ è½½æ›´å¤šâ€ï¼Œç›´åˆ°å…ƒç´ æ•°é‡ç¨³å®š
            logger.info("æ­£åœ¨æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹...")
            stable_loops = 0
            last_count = -1
            max_loops = 80

            def query_item_count():
                try:
                    return int(self.driver.execute_script(
                        "return document.querySelectorAll(`div.item, div[class*=item], div[class*=event], li[class*=item], li[class*=event], .timeline-item, .event-item`).length;")
                    )
                except Exception:
                    return 0

            # è‹¥é¡µé¢æä¾›æ€»æ•°ï¼Œä½œä¸ºé€€å‡ºå‚è€ƒ
            declared_total = 0
            try:
                declared_total = int(self.core_info.get('sub_event_count', 0))
            except Exception:
                declared_total = 0

            for i in range(max_loops):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)

                # å°è¯•ç‚¹å‡»â€œåŠ è½½æ›´å¤š/å±•å¼€â€
                try:
                    load_buttons = self.driver.find_elements(By.XPATH, "//button[contains(., 'åŠ è½½') or contains(., 'æ›´å¤š') or contains(translate(., 'MORE', 'more'), 'more') or contains(translate(., 'LOAD', 'load'), 'load')] | //*[(contains(@class, 'load') or contains(@class, 'more')) and self::button] | //a[contains(., 'åŠ è½½') or contains(., 'æ›´å¤š')]")
                    clicked = False
                    for btn in load_buttons:
                        if btn.is_displayed() and btn.is_enabled():
                            try:
                                self.driver.execute_script("arguments[0].click();", btn)
                                clicked = True
                                time.sleep(2)
                            except Exception:
                                continue
                    if clicked:
                        time.sleep(1)
                except Exception:
                    pass

                count_now = query_item_count()
                logger.debug(f"åŠ è½½å¾ªç¯ {i+1}: å½“å‰äº‹ä»¶é¡¹ {count_now}")

                if count_now == last_count:
                    stable_loops += 1
                else:
                    stable_loops = 0
                last_count = count_now

                # é€€å‡ºæ¡ä»¶ï¼šç¨³å®šå¤šæ¬¡æˆ–è¾¾åˆ°å£°æ˜æ€»æ•°
                if (declared_total and count_now >= declared_total) or stable_loops >= 5:
                    break

            # æœ€åç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½å®Œæˆ
            time.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            event_items = []
            selectors = [
                'div.item',
                'div[class*="item"]',
                'div[class*="event"]',
                'li[class*="item"]',
                'li[class*="event"]',
                '.timeline-item',
                '.event-item',
                'section[class*="item"]',
            ]
            for selector in selectors:
                items = soup.select(selector)
                if items and len(items) > len(event_items):
                    event_items = items
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(items)} ä¸ªäº‹ä»¶é¡¹")
            
            logger.info(f"æœ€ç»ˆæ‰¾åˆ° {len(event_items)} ä¸ªäº‹ä»¶é¡¹")
            
            seen_keys = set()
            for i, item in enumerate(event_items):
                try:
                    sub_event = self._extract_event_from_item(item, i+1)
                    if sub_event:
                        key = (sub_event.get('title', ''), sub_event.get('time', ''))
                        if key not in seen_keys:
                            seen_keys.add(key)
                            self.sub_events.append(sub_event)
                except Exception as e:
                    logger.warning(f"è§£æäº‹ä»¶é¡¹ {i+1} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"æˆåŠŸæå– {len(self.sub_events)} ä¸ªå­äº‹ä»¶")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå­äº‹ä»¶é¢„è§ˆ
            for i, event in enumerate(self.sub_events[:5]):
                logger.info(f"å­äº‹ä»¶ {i+1}: {event['title'][:50]}... - {event['time']}")
            
            return True
            
        except Exception as e:
            logger.error(f"çˆ¬å–å­äº‹ä»¶å¤±è´¥: {e}")
            return False
    
    def _extract_event_from_item(self, item, index):
        """ä»å•ä¸ªitemå…ƒç´ ä¸­æå–äº‹ä»¶ä¿¡æ¯"""
        sub_event = {
            'id': f'event_{index}',
            'title': '',
            'link': '',
            'time': '',
            'summary': '',
            'author': ''
        }
        
        # æå–æ—¶é—´
        time_elem = item.find('span', class_='time')
        if time_elem:
            sub_event['time'] = time_elem.get_text(strip=True)
        
        # æå–æ ‡é¢˜å’Œé“¾æ¥
        title_link = item.find('a', class_='content-link')
        if title_link:
            sub_event['title'] = title_link.get_text(strip=True)
            sub_event['link'] = title_link.get('href', '')
        
        # æå–æ‘˜è¦å’Œä½œè€…
        dynamic_container = item.find('a', class_='dynamic-container')
        if dynamic_container:
            # æå–ä½œè€…
            author_elem = dynamic_container.find('div', class_='dynamic-author')
            if author_elem:
                author_text = author_elem.get_text(strip=True)
                sub_event['author'] = author_text.replace('ï¼š', '').replace(':', '')
            
            # æå–æ‘˜è¦
            content_elem = dynamic_container.find('div', class_='dynamic-content')
            if content_elem:
                sub_event['summary'] = content_elem.get_text(strip=True)
        
        # åªæœ‰å½“æ ‡é¢˜ä¸ä¸ºç©ºæ—¶æ‰è¿”å›
        if sub_event['title']:
            return sub_event
        
        return None
    
    def save_data(self, filename='data/level1_data.json'):
        """ä¿å­˜æ•°æ®"""
        try:
            output_data = {
                'core_info': self.core_info,
                'sub_events': self.sub_events,
                'total_sub_events': len(self.sub_events),
                'scrape_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def start_level2_scraping(self, output_dir: str = None, csv_output_file: str = None):
        """å¯åŠ¨äºŒçº§è¯„è®ºçˆ¬å–"""
        logger.info("å¼€å§‹å¯åŠ¨äºŒçº§è¯„è®ºçˆ¬å–...")
        
        try:
            # åˆ›å»ºäºŒçº§çˆ¬è™«å®ä¾‹
            level2_scraper = Level2Scraper(
                self.core_info.get('core_event_name', ''),
                output_dir=output_dir,
                csv_output_file=csv_output_file
            )
            
            # å¼€å§‹çˆ¬å–è¯„è®º
            total_comments = level2_scraper.scrape_all_comments(self.sub_events)
            
            # æ˜¾ç¤ºæ‘˜è¦
            level2_scraper.print_summary()
            
            # å…³é—­èµ„æº
            level2_scraper.close()
            
            # ç”Ÿæˆåˆå¹¶æ•°æ®åˆ°åŒä¸€ç›®å½•
            try:
                dm = DataManager(output_dir or 'data')
                dm.combine_data()
            except Exception as _:
                logger.warning("åˆå¹¶æ•°æ®å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹")

            logger.info(f"äºŒçº§è¯„è®ºçˆ¬å–å®Œæˆï¼Œå…±è·å– {total_comments} æ¡è¯„è®º")
            return total_comments
            
        except Exception as e:
            logger.error(f"äºŒçº§è¯„è®ºçˆ¬å–å¤±è´¥: {e}")
            return 0
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ä¸€çº§ç•Œé¢çˆ¬å–ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ ¸å¿ƒäº‹ä»¶: {self.core_info.get('core_event_name', 'æœªè·å–')}")
        print(f"æ›´æ–°æ—¶é—´: {self.core_info.get('update_time', 'æœªè·å–')}")
        print(f"å­äº‹ä»¶æ•°é‡: {self.core_info.get('sub_event_count', 0)}")
        print(f"å®é™…æå–: {len(self.sub_events)} ä¸ªå­äº‹ä»¶")
        
        if self.sub_events:
            print(f"\nå‰5ä¸ªå­äº‹ä»¶:")
            for i, event in enumerate(self.sub_events[:5]):
                print(f"{i+1}. {event['time']} - {event['title'][:50]}...")
                if event['author']:
                    print(f"   ä½œè€…: {event['author']}")
                if event['summary']:
                    print(f"   æ‘˜è¦: {event['summary'][:80]}...")
                print()
        
        print("="*60)
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.driver:
            self.driver.quit()
        self.session.close()

def main():
    """ä¸»å‡½æ•°"""
    # å¯æ›¿æ¢ä¸ºå…¶ä»–æ ¸å¿ƒäº‹ä»¶é¡µé¢URL
    target_url = "https://events.baidu.com/search/vein?platform=pc&record_id=648521&query=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E6%8A%97%E6%97%A5%E6%88%98%E4%BA%89%E6%9A%A8%E4%B8%96%E7%95%8C%E5%8F%8D%E6%B3%95%E8%A5%BF%E6%96%AF%E6%88%98%E4%BA%89%E8%83%9C%E5%88%A980%E5%91%A8%E5%B9%B4%E7%BA%AA%E5%BF%B5%E6%97%A5&srcid=50367"
    
    scraper = Level1Scraper()
    try:
        # çˆ¬å–æ ¸å¿ƒä¿¡æ¯
        if scraper.scrape_core_info(target_url):
            # çˆ¬å–å­äº‹ä»¶
            if scraper.scrape_sub_events(target_url):
                scraper.save_data()
                print(f"\nâœ… ä¸€çº§ç•Œé¢çˆ¬å–å®Œæˆ!")
                print(f"ğŸ“Š æ ¸å¿ƒäº‹ä»¶: {scraper.core_info['core_event_name']}")
                print(f"ğŸ“Š æ›´æ–°æ—¶é—´: {scraper.core_info['update_time']}")
                print(f"ğŸ“Š å­äº‹ä»¶æ•°é‡: {scraper.core_info['sub_event_count']}")
                print(f"ğŸ“Š å®é™…æå–: {len(scraper.sub_events)} ä¸ªå­äº‹ä»¶")
                print(f"ğŸ“„ æ•°æ®ä¿å­˜: data/level1_data.json")
                
                # æ˜¾ç¤ºæ‘˜è¦
                scraper.print_summary()
                
                # è‡ªåŠ¨å¯åŠ¨äºŒçº§çˆ¬å–ï¼ˆå»é™¤äº¤äº’ï¼‰
                print(f"\nğŸš€ å¼€å§‹çˆ¬å–è¯„è®º...")
                total_comments = scraper.start_level2_scraping()
                print(f"\nğŸ‰ å®Œæ•´çˆ¬å–å®Œæˆï¼")
                print(f"ğŸ“Š æ€»è¯„è®ºæ•°: {total_comments}")
                print(f"ğŸ“„ JSONæ–‡ä»¶: data/level2_data.json")
                print(f"ğŸ“Š Excelæ–‡ä»¶: data/{scraper._sanitize_filename(scraper.core_info['core_event_name'])}_è¯„è®ºæ•°æ®.xlsx")
            else:
                print("âŒ å­äº‹ä»¶çˆ¬å–å¤±è´¥")
        else:
            print("âŒ æ ¸å¿ƒä¿¡æ¯çˆ¬å–å¤±è´¥")
        
    finally:
        scraper.close()

if __name__ == "__main__":
    main()
