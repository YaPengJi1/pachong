#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„çˆ¬è™«
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from level1_scraper import Level1Scraper
from level2_scraper import Level2Scraper
from data_manager import DataManager
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('test_scraper.log', encoding='utf-8')
    ]
)

def test_scraper():
    """æµ‹è¯•ä¿®å¤åçš„çˆ¬è™«"""
    logger = logging.getLogger(__name__)
    
    # æµ‹è¯•URL
    test_url = "https://events.baidu.com/search/vein?platform=pc&record_id=648521&query=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E6%8A%97%E6%97%A5%E6%88%98%E4%BA%89%E6%9A%A8%E4%B8%96%E7%95%8C%E5%8F%8D%E6%B3%95%E8%A5%BF%E6%96%AF%E6%88%98%E4%BA%89%E8%83%9C%E5%88%A980%E5%91%A8%E5%B9%B4%E7%BA%AA%E5%BF%B5%E6%97%A5&srcid=50367"
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        logger.info("åˆå§‹åŒ–çˆ¬è™«...")
        scraper1 = Level1Scraper()
        scraper2 = Level2Scraper()
        
        # æµ‹è¯•æ ¸å¿ƒä¿¡æ¯çˆ¬å–
        logger.info("æµ‹è¯•æ ¸å¿ƒä¿¡æ¯çˆ¬å–...")
        if scraper1.scrape_core_info(test_url):
            logger.info("âœ… æ ¸å¿ƒä¿¡æ¯çˆ¬å–æˆåŠŸ")
            logger.info(f"æ ¸å¿ƒä¿¡æ¯: {scraper1.core_info}")
        else:
            logger.error("âŒ æ ¸å¿ƒä¿¡æ¯çˆ¬å–å¤±è´¥")
            return False
        
        # æµ‹è¯•å­äº‹ä»¶çˆ¬å–ï¼ˆé™åˆ¶æ•°é‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼‰
        logger.info("æµ‹è¯•å­äº‹ä»¶çˆ¬å–ï¼ˆé™åˆ¶å‰5ä¸ªï¼‰...")
        if scraper1.scrape_sub_events(test_url):
            logger.info(f"âœ… å­äº‹ä»¶çˆ¬å–æˆåŠŸï¼Œå…±æ‰¾åˆ° {len(scraper1.sub_events)} ä¸ªå­äº‹ä»¶")
            
            # æ˜¾ç¤ºå‰5ä¸ªå­äº‹ä»¶
            for i, event in enumerate(scraper1.sub_events[:5]):
                logger.info(f"  å­äº‹ä»¶ {i+1}: {event.get('title', 'N/A')}")
        else:
            logger.error("âŒ å­äº‹ä»¶çˆ¬å–å¤±è´¥")
            return False
        
        # æµ‹è¯•è¯„è®ºçˆ¬å–ï¼ˆåªæµ‹è¯•ç¬¬ä¸€ä¸ªå­äº‹ä»¶ï¼‰
        if scraper1.sub_events:
            logger.info("æµ‹è¯•è¯„è®ºçˆ¬å–ï¼ˆç¬¬ä¸€ä¸ªå­äº‹ä»¶ï¼‰...")
            first_event = scraper1.sub_events[0]
            event_url = first_event.get('url')
            
            if event_url:
                logger.info(f"æµ‹è¯•URL: {event_url}")
                comments = scraper2.scrape_comments(event_url, first_event.get('title', ''), first_event.get('id', ''))
                
                if comments:
                    logger.info(f"âœ… è¯„è®ºçˆ¬å–æˆåŠŸï¼Œå…±æ‰¾åˆ° {len(comments)} æ¡è¯„è®º")
                    
                    # æ˜¾ç¤ºå‰3æ¡è¯„è®º
                    for i, comment in enumerate(comments[:3]):
                        logger.info(f"  è¯„è®º {i+1}: {comment.get('comment_content', 'N/A')[:50]}...")
                else:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°è¯„è®º")
            else:
                logger.warning("âš ï¸ ç¬¬ä¸€ä¸ªå­äº‹ä»¶æ²¡æœ‰URL")
        
        logger.info("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False
    
    finally:
        # æ¸…ç†èµ„æº
        try:
            scraper1.close()
            scraper2.close()
        except:
            pass

if __name__ == "__main__":
    success = test_scraper()
    if success:
        print("\nâœ… æµ‹è¯•æˆåŠŸï¼çˆ¬è™«ä¿®å¤æœ‰æ•ˆã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
