#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ£€æµ‹ç™¾åº¦äº‹ä»¶é¡µé¢çš„æœ‰æ•ˆrecord_idï¼ˆä¼˜åŒ–ç‰ˆï¼‰
- å¹¶è¡Œæ£€æµ‹
- æ—¶é—´è¿‡æ»¤ï¼ˆåªä¿ç•™2025å¹´1æœˆ1æ—¥ä¹‹åçš„äº‹ä»¶ï¼‰
- å®æ—¶ä¿å­˜ç»“æœ
"""

import requests
import concurrent.futures
import time
import json
import re
import threading
import csv
from datetime import datetime, date
from bs4 import BeautifulSoup
import os

class RecordIdChecker:
    def __init__(self, output_file='valid_record_ids.csv'):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.baidu.com/',
        })
        
        self.output_file = output_file
        self.valid_ids = []
        self.new_records = []  # æ–°å‘ç°çš„è®°å½•
        self.lock = threading.Lock()
        self.min_date = date(2025, 1, 1)  # æœ€å°æ—¥æœŸï¼š2025å¹´1æœˆ1æ—¥
        
        # åŠ è½½å·²ä¿å­˜çš„ç»“æœ
        self.load_existing_results()
    
    def load_existing_results(self):
        """åŠ è½½å·²ä¿å­˜çš„ç»“æœ"""
        try:
            if os.path.exists(self.output_file):
                # å°è¯•è¯»å–æ–‡ä»¶ï¼Œå¤„ç†NULå­—ç¬¦é—®é¢˜
                with open(self.output_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    # ç§»é™¤NULå­—ç¬¦
                    content = content.replace('\x00', '')
                
                # é‡æ–°å†™å…¥æ¸…ç†åçš„å†…å®¹
                with open(self.output_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                # é‡æ–°è¯»å–
                with open(self.output_file, 'r', encoding='utf-8', newline='') as f:
                    reader = csv.DictReader(f)
                    self.valid_ids = list(reader)
                print(f"ğŸ“ å·²åŠ è½½ {len(self.valid_ids)} ä¸ªå·²ä¿å­˜çš„æœ‰æ•ˆID")
            else:
                self.valid_ids = []
                print("ğŸ“ æœªæ‰¾åˆ°å·²ä¿å­˜çš„ç»“æœæ–‡ä»¶ï¼Œä»å¤´å¼€å§‹")
        except Exception as e:
            print(f"âŒ åŠ è½½å·²æœ‰ç»“æœå¤±è´¥: {e}")
            print("ğŸ”„ åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶...")
            self.valid_ids = []
    
    def translate_to_english(self, chinese_text):
        """ç¿»è¯‘ä¸­æ–‡åˆ°è‹±æ–‡"""
        try:
            # ç®€å•çš„ç¿»è¯‘æ˜ å°„ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
            translations = {
                'iPhone17è„‰ç»œå¡': 'iPhone17_Timeline',
                '2023å¹´10æœˆå·´ä»¥å†²çª': 'October_2023_Israel_Palestine_Conflict',
                'æŠ—æ—¥æˆ˜äº‰æš¨åæ³•è¥¿æ–¯æˆ˜äº‰èƒœåˆ©80å‘¨å¹´': '80th_Anniversary_of_Victory_in_Anti_Japanese_War_and_World_Anti_Fascist_War',
                'ç¾å›½æ‰€è°“å¯¹ç­‰å…³ç¨æ”¿ç­–': 'US_Reciprocal_Tariff_Policy',
                'é‚£è‹±è€å…¬å¦è®¤å‡ºè½¨': 'Na_Ying_Husband_Denies_Cheating',
                'æ–°è¥¿å…°å¤®è¡Œé™æ¯å‘¨æœŸå¼€å¯': 'New_Zealand_Central_Bank_Interest_Rate_Cut_Cycle_Begins',
                'å› è…¿ä¼¤è¢«æ€æ‰¶ä¸Šè½¦': 'Helped_into_Car_Due_to_Leg_Injury',
                'ç‰¹æœ—æ™®ä¸æ™®äº¬è°ˆåˆ¤ç¾ä¿„ä¹Œä¸‰æ–¹ä¼šæ™¤': 'Trump_Putin_Negotiations_US_Russia_Ukraine_Tripartite_Meeting'
            }
            
            # å¦‚æœæ‰¾åˆ°ç›´æ¥æ˜ å°„ï¼Œè¿”å›
            if chinese_text in translations:
                return translations[chinese_text]
            
            # å°è¯•éƒ¨åˆ†åŒ¹é…
            for key, value in translations.items():
                if key in chinese_text:
                    return value
            
            # å¦åˆ™è¿”å›åŸæ ‡é¢˜ï¼ˆå¯ä»¥åç»­æ¥å…¥ç¿»è¯‘APIï¼‰
            return chinese_text
            
        except Exception as e:
            return chinese_text
    
    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            print(f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(self.valid_ids)} æ¡è®°å½•åˆ° {self.output_file}")
            
            # æ¸…ç†æ•°æ®ä¸­çš„NULå­—ç¬¦
            cleaned_data = []
            for item in self.valid_ids:
                cleaned_item = {}
                for key, value in item.items():
                    if isinstance(value, str):
                        cleaned_item[key] = value.replace('\x00', '')
                    else:
                        cleaned_item[key] = value
                cleaned_data.append(cleaned_item)
            
            # ç®€åŒ–ä¿å­˜é€»è¾‘ï¼Œä¸ä½¿ç”¨é”
            with open(self.output_file, 'w', encoding='utf-8', newline='') as f:
                print(f"ğŸ“ æ–‡ä»¶åˆ›å»º/æ‰“å¼€æˆåŠŸ")
                fieldnames = ['url', 'title_chinese', 'title_english', 'update_date', 'found_time']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å†™å…¥è¡¨å¤´
                writer.writeheader()
                print(f"ğŸ“‹ è¡¨å¤´å†™å…¥å®Œæˆ")
                
                # å†™å…¥æ•°æ®
                for i, item in enumerate(cleaned_data):
                    writer.writerow(item)
                    print(f"ğŸ“Š å†™å…¥ç¬¬ {i+1} æ¡è®°å½•: {item['title_chinese']}")
                
                print(f"âœ… å…¨éƒ¨ {len(cleaned_data)} æ¡è®°å½•ä¿å­˜å®Œæˆï¼")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def extract_update_time(self, content):
        """æå–æ›´æ–°æ—¶é—´"""
        try:
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(content, 'html.parser')
            
            # æŸ¥æ‰¾æ›´æ–°æ—¶é—´å…ƒç´ 
            time_elem = soup.find('p', class_='create-time')
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                print(f"ğŸ” æ‰¾åˆ°æ—¶é—´å…ƒç´ : {time_text}")  # è°ƒè¯•ä¿¡æ¯
                
                # æ ¼å¼1: "æ›´æ–°è‡³2025å¹´9æœˆ10æ—¥ 10:08"
                date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', time_text)
                if date_match:
                    year, month, day = map(int, date_match.groups())
                    return date(year, month, day)
                
                # æ ¼å¼2: "æ›´æ–°è‡³20250820"
                date_match = re.search(r'æ›´æ–°è‡³(\d{8})', time_text)
                if date_match:
                    date_str = date_match.group(1)
                    year = int(date_str[:4])
                    month = int(date_str[4:6])
                    day = int(date_str[6:8])
                    return date(year, month, day)
            
            # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥åœ¨å†…å®¹ä¸­æœç´¢
            # æ ¼å¼1: "æ›´æ–°è‡³2025å¹´9æœˆ10æ—¥"
            date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', content)
            if date_match:
                year, month, day = map(int, date_match.groups())
                return date(year, month, day)
            
            # æ ¼å¼2: "æ›´æ–°è‡³20250820"
            date_match = re.search(r'æ›´æ–°è‡³(\d{8})', content)
            if date_match:
                date_str = date_match.group(1)
                year = int(date_str[:4])
                month = int(date_str[4:6])
                day = int(date_str[6:8])
                return date(year, month, day)
                
        except Exception as e:
            print(f"âŒ æ—¶é—´è§£æé”™è¯¯: {e}")
        
        return None
    
    def check_single_id(self, record_id):
        """æ£€æŸ¥å•ä¸ªID"""
        url = f"https://events.baidu.com/search/vein?platform=pc&record_id={record_id}"
        
        try:
            print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ ID: {record_id}")
            response = self.session.get(url, timeout=10)
            print(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                content = response.text
                print(f"ğŸ“„ é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«äº‹ä»¶é¡µé¢ç‰¹å¾
                if any(keyword in content for keyword in ['æ›´æ–°è‡³', 'å…¨éƒ¨', 'æ—¶é—´å€’åº']):
                    print(f"âœ… ID {record_id} åŒ…å«äº‹ä»¶é¡µé¢ç‰¹å¾")
                    
                    # æå–æ›´æ–°æ—¶é—´
                    print(f"ğŸ• å¼€å§‹æå–æ—¶é—´...")
                    update_date = self.extract_update_time(content)
                    print(f"ğŸ“… è§£æåˆ°æ—¶é—´: {update_date}")
                    
                    # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™2025å¹´1æœˆ1æ—¥ä¹‹åçš„äº‹ä»¶
                    if update_date and update_date >= self.min_date:
                        print(f"âœ… æ—¶é—´ç¬¦åˆè¦æ±‚ï¼Œå¼€å§‹æå–æ ‡é¢˜...")
                        title = self.extract_title(content)
                        print(f"ğŸ“ æå–åˆ°æ ‡é¢˜: {title}")
                        
                        print(f"ğŸŒ å¼€å§‹ç¿»è¯‘...")
                        title_english = self.translate_to_english(title)
                        print(f"ğŸŒ ç¿»è¯‘ç»“æœ: {title_english}")
                        
                        result = {
                            'url': url,
                            'title_chinese': title,
                            'title_english': title_english,
                            'update_date': update_date.isoformat(),
                            'found_time': datetime.now().isoformat()
                        }
                        
                        print(f"ğŸ’¾ æ·»åŠ åˆ°æ–°è®°å½•åˆ—è¡¨...")
                        # æ·»åŠ åˆ°æ–°è®°å½•åˆ—è¡¨ï¼Œä¸ç«‹å³ä¿å­˜
                        self.new_records.append(result)
                        
                        print(f"âœ… ID {record_id} æ·»åŠ åˆ°æ–°è®°å½•åˆ—è¡¨!")
                        return result
                    else:
                        reason = f'æ—¶é—´è¿‡æ—§: {update_date}' if update_date else 'æ— æ³•è§£ææ—¶é—´'
                        print(f"â° ID {record_id} æ—¶é—´è¿‡æ»¤: {reason}")
                    return {
                        'id': record_id,
                            'status': 'filtered',
                            'reason': reason
                    }
                else:
                    print(f"âŒ ID {record_id} ä¸åŒ…å«äº‹ä»¶é¡µé¢ç‰¹å¾")
                        
        except Exception as e:
            print(f"âŒ ID {record_id} è¯·æ±‚å¤±è´¥: {e}")
        
        return {'id': record_id, 'status': 'invalid'}
    
    def extract_title(self, content):
        """æå–é¡µé¢æ ‡é¢˜"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            title_elem = soup.find('title')
            if title_elem:
                return title_elem.get_text(strip=True)
            
            # å¤‡ç”¨æ–¹æ³•
            title_match = re.search(r'<title>(.*?)</title>', content)
            return title_match.group(1) if title_match else 'Unknown'
        except:
            return 'Unknown'
    
    def batch_check(self, start_id=591500, end_id=800000, max_workers=15, use_parallel=False, batch_size=100):
        """æ‰¹é‡æ£€æµ‹"""
        print(f"ğŸ” å¼€å§‹æ£€æµ‹ record_id {start_id} åˆ° {end_id}")
        print(f"ğŸ“… åªä¿ç•™ {self.min_date} ä¹‹åçš„äº‹ä»¶")
        print(f"ğŸ’¾ ç»“æœå®æ—¶ä¿å­˜åˆ° {self.output_file}")
        print(f"ğŸ”„ å¤„ç†æ¨¡å¼: {'å¹¶è¡Œ' if use_parallel else 'ä¸²è¡Œ'}")
        if use_parallel:
            print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size} (æ¯{batch_size}ä¸ªIDæš‚åœæ•´ç†ä¸€æ¬¡)")
        
        total_checked = 0
        valid_count = 0
        filtered_count = 0
        
        if use_parallel and max_workers > 1:
            # å¹¶è¡Œå¤„ç†æ¨¡å¼ - åˆ†æ‰¹å¤„ç†
            print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†")
            
            for batch_start in range(start_id, end_id + 1, batch_size):
                batch_end = min(batch_start + batch_size - 1, end_id)
                print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡: {batch_start} - {batch_end}")
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                    future_to_id = {
                        executor.submit(self.check_single_id, i): i 
                        for i in range(batch_start, batch_end + 1)
                    }
                    
                    # å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
                    batch_results = []
                    for future in concurrent.futures.as_completed(future_to_id):
                        result = future.result()
                        batch_results.append(result)
                        total_checked += 1
                        
                        if result.get('status') == 'valid':
                            valid_count += 1
                            print(f"âœ… [{total_checked}/{end_id-start_id+1}] å‘ç°æœ‰æ•ˆID: {result['id']}")
                        elif result.get('status') == 'filtered':
                            filtered_count += 1
                            print(f"â° [{total_checked}/{end_id-start_id+1}] æ—¶é—´è¿‡æ»¤: {result['id']} - {result['reason']}")
                        elif result.get('status') == 'invalid':
                            print(f"âŒ [{total_checked}/{end_id-start_id+1}] æ— æ•ˆID: {result['id']}")
                        else:
                            # ç›´æ¥ä¿å­˜çš„ç»“æœ
                            valid_count += 1
                            print(f"âœ… [{total_checked}/{end_id-start_id+1}] å‘ç°æœ‰æ•ˆID: {result.get('id', 'unknown')}")
                    
                    # æ‰¹æ¬¡å®Œæˆï¼Œæ•´ç†æ’åº
                    print(f"ğŸ”„ æ‰¹æ¬¡å®Œæˆï¼Œå¼€å§‹æ•´ç†æ’åº...")
                    self.sort_and_save_results()
                    print(f"âœ… æ’åºå®Œæˆï¼Œç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡")
                    
                    # è¿›åº¦æ˜¾ç¤º
                    print(f"ğŸ“Š å·²æ£€æµ‹ {total_checked}/{end_id-start_id+1} ä¸ªIDï¼Œæœ‰æ•ˆ: {valid_count}ï¼Œè¿‡æ»¤: {filtered_count}")
        else:
            # ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            print(f"ğŸ”„ ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼")
            for record_id in range(start_id, end_id + 1):
                print(f"\nğŸ”„ å¤„ç† ID {record_id} ({total_checked + 1}/{end_id-start_id+1})")
                result = self.check_single_id(record_id)
                total_checked += 1
                
                if 'status' in result:
                    if result['status'] == 'valid':
                        valid_count += 1
                        print(f"âœ… å‘ç°æœ‰æ•ˆID: {record_id}")
                    elif result['status'] == 'filtered':
                        filtered_count += 1
                        print(f"â° æ—¶é—´è¿‡æ»¤: {record_id} - {result['reason']}")
                    else:
                        print(f"âŒ æ— æ•ˆID: {record_id}")
                else:
                    # ç›´æ¥ä¿å­˜çš„ç»“æœ
                    valid_count += 1
                    print(f"âœ… å‘ç°æœ‰æ•ˆID: {record_id}")
        
        print(f"\nğŸ‰ æ£€æµ‹å®Œæˆï¼")
        print(f"ğŸ“Š æ€»æ£€æµ‹: {total_checked} ä¸ªID")
        print(f"âœ… æœ‰æ•ˆID: {valid_count} ä¸ª")
        print(f"â° æ—¶é—´è¿‡æ»¤: {filtered_count} ä¸ª")
        
        # æœ€åä¿å­˜æ‰€æœ‰æ–°å‘ç°çš„è®°å½•
        if hasattr(self, 'new_records') and self.new_records:
            print(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ‰€æœ‰æ–°å‘ç°çš„ {len(self.new_records)} æ¡è®°å½•...")
            self.sort_and_save_results()
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {self.output_file}")
        
        return self.valid_ids
    
    def sort_and_save_results(self):
        """åªå¯¹æ–°å‘ç°çš„è®°å½•è¿›è¡Œæ’åºå’Œè¿½åŠ ä¿å­˜"""
        try:
            print(f"ğŸ”„ å¼€å§‹å¤„ç†æ–°å‘ç°çš„è®°å½•...")
            
            # åªå¤„ç†æ–°å‘ç°çš„è®°å½•ï¼Œä¸é‡æ–°æ’åºæ‰€æœ‰è®°å½•
            if hasattr(self, 'new_records') and self.new_records:
                print(f"ğŸ“ å‘ç° {len(self.new_records)} æ¡æ–°è®°å½•ï¼Œè¿½åŠ ä¿å­˜")
                
                # æŒ‰record_idæ’åºæ–°è®°å½•
                def extract_record_id(url):
                    try:
                        match = re.search(r'record_id=(\d+)', url)
                        return int(match.group(1)) if match else 0
                    except:
                        return 0
                
                self.new_records.sort(key=lambda x: extract_record_id(x['url']))
                print(f"âœ… æ–°è®°å½•æ’åºå®Œæˆ")
                
                # è¿½åŠ ä¿å­˜æ–°è®°å½•
                self.append_new_records()
            else:
                print(f"â„¹ï¸ æ²¡æœ‰æ–°è®°å½•éœ€è¦ä¿å­˜")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–°è®°å½•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def append_new_records(self):
        """è¿½åŠ æ–°è®°å½•åˆ°CSVæ–‡ä»¶"""
        try:
            if not hasattr(self, 'new_records') or not self.new_records:
                return
                
            print(f"ğŸ’¾ å¼€å§‹è¿½åŠ  {len(self.new_records)} æ¡æ–°è®°å½•åˆ° {self.output_file}")
            
            # æ¸…ç†æ•°æ®ä¸­çš„NULå­—ç¬¦
            cleaned_data = []
            for item in self.new_records:
                cleaned_item = {}
                for key, value in item.items():
                    if isinstance(value, str):
                        cleaned_item[key] = value.replace('\x00', '')
                    else:
                        cleaned_item[key] = value
                cleaned_data.append(cleaned_item)
            
            # è¿½åŠ æ¨¡å¼å†™å…¥
            with open(self.output_file, 'a', encoding='utf-8', newline='') as f:
                fieldnames = ['url', 'title_chinese', 'title_english', 'update_date', 'found_time']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å†™å…¥æ–°æ•°æ®
                for i, item in enumerate(cleaned_data):
                    writer.writerow(item)
                    print(f"ğŸ“Š è¿½åŠ ç¬¬ {i+1} æ¡è®°å½•: {item['title_chinese']}")
                
                print(f"âœ… å…¨éƒ¨ {len(cleaned_data)} æ¡æ–°è®°å½•è¿½åŠ å®Œæˆï¼")
            
            # æ¸…ç©ºæ–°è®°å½•åˆ—è¡¨
            self.new_records = []
                
        except Exception as e:
            print(f"âŒ è¿½åŠ ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    # åˆ›å»ºæ£€æµ‹å™¨å®ä¾‹
    checker = RecordIdChecker('valid_record_ids.csv')
    
    # æ£€æµ‹èŒƒå›´ï¼ˆå»ºè®®å…ˆå°èŒƒå›´æµ‹è¯•ï¼‰
    print("ğŸš€ å¼€å§‹æ‰¹é‡æ£€æµ‹...")
    print("ğŸ’¡ å»ºè®®å…ˆç”¨å°èŒƒå›´æµ‹è¯•ï¼Œå¦‚ start_id=1, end_id=1000")
    
    # å¯ä»¥è°ƒæ•´æ£€æµ‹èŒƒå›´å’Œå¹¶è¡Œè®¾ç½®
    # ä¸²è¡Œæ¨¡å¼ï¼ˆæ¨èï¼Œç¨³å®šï¼‰
    #valid_ids = checker.batch_check(start_id=701198, end_id=701198, max_workers=1, use_parallel=False)
    
    # å¹¶è¡Œæ¨¡å¼ï¼ˆå¯é€‰ï¼Œé€Ÿåº¦å¿«ä½†å¯èƒ½ä¸ç¨³å®šï¼‰
    valid_ids = checker.batch_check(start_id=592000, end_id=800000, max_workers=15, use_parallel=True, batch_size=1000)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"âœ… æ‰¾åˆ° {len(valid_ids)} ä¸ª2025å¹´1æœˆ1æ—¥ä¹‹åçš„æœ‰æ•ˆID")
    
    # æ˜¾ç¤ºç»“æœ
    if valid_ids:
        print(f"\nğŸ“‹ æ‰¾åˆ°çš„æœ‰æ•ˆID:")
        for item in valid_ids:
            print(f"  URL: {item['url']}")
            print(f"  ä¸­æ–‡æ ‡é¢˜: {item['title_chinese']}")
            print(f"  è‹±æ–‡æ ‡é¢˜: {item['title_english']}")
            print(f"  æ›´æ–°æ—¶é—´: {item['update_date']}")
            print(f"  å‘ç°æ—¶é—´: {item['found_time']}")
            print("  " + "-"*50)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° valid_record_ids.csv")

if __name__ == "__main__":
    main()